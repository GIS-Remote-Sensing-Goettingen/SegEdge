{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tifffile import imread"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5d22deece30c713e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"facebook/dinov3-vitl16-pretrain-sat493m\"  # smaller + sat-trained\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
    "\n",
    "print(\"device:\", device)"
   ],
   "id": "741fca179ce67f1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#load incomplite unrefined labels\n",
    "\n",
    "labels = imread(\"/run/media/mak/Partition of 1TB disk/SH_dataset/planet_labels_2022.tif\")\n",
    "image = imread(\"/home/mak/PycharmProjects/SegEdge/experiments/get_data_from_api/patches_mt/dop20_593000_5982000_1km_20cm.tif\")\n",
    "\n",
    "image.shape, labels.shape"
   ],
   "id": "38528dce01175d4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "from rasterio.windows import Window\n",
    "\n",
    "img_path = \"/home/mak/PycharmProjects/SegEdge/experiments/get_data_from_api/patches_mt/dop20_593000_5982000_1km_20cm.tif\"\n",
    "\n",
    "with rasterio.open(img_path) as img1:\n",
    "    # Basic metadata\n",
    "    crs = img1.crs                 # e.g., EPSG:25832 for German DOP20 tiles [web:60]\n",
    "\n",
    "    transform = img1.transform     # Affine mapping pixel -> map coords [web:60]\n",
    "    width, height = img1.width, img1.height  # pixel dimensions [web:61]\n",
    "    count = img1.count             # number of bands [web:61]\n",
    "    bounds = img1.bounds           # left, bottom, right, top in CRS units [web:40]\n",
    "\n",
    "    # Read all bands into array shaped (bands, H, W)\n",
    "    arr = img1.read()              # preserves dtype; no normalization [web:40]\n",
    "    print(\"crs:\", crs, \" transform:\", transform, \" size:\", (width, height), \" bands:\", count)\n",
    "\n",
    "# If you want HxWxC for visualization:\n",
    "img = reshape_as_image(arr)       # converts (C,H,W) -> (H,W,C) [web:40]\n",
    "\n",
    "#print image\n",
    "plt.imshow(img)"
   ],
   "id": "113a4c1d3a5c193f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "\n",
    "lab_path = \"/run/media/mak/Partition of 1TB disk/SH_dataset/planet_labels_2022.tif\"\n",
    "\n",
    "with rasterio.open(img_path) as ref, rasterio.open(lab_path) as img1:\n",
    "    dst_meta = ref.meta.copy()\n",
    "    dst_meta.update(dtype=img1.dtypes[0], count=img1.count)  # keep label dtype/bands [web:65]\n",
    "    labels_on_img = rasterio.io.MemoryFile().open(**dst_meta)\n",
    "    for i in range(1, img1.count + 1):\n",
    "        dest = rasterio.band(labels_on_img, i)\n",
    "        reproject(\n",
    "            source=rasterio.band(img1, i),\n",
    "            destination=dest,\n",
    "            src_transform=img1.transform, src_crs=img1.crs,\n",
    "            dst_transform=ref.transform, dst_crs=ref.crs,\n",
    "            dst_width=ref.width, dst_height=ref.height,\n",
    "            resampling=Resampling.nearest,  # categorical labels [web:65][web:74]\n",
    "        )\n",
    "    # read to array if needed\n",
    "    labels_arr = labels_on_img.read()  # shape (bands, H, W) aligned to image [web:65]\n",
    "    print(\"reprojected labels shape:\", labels_arr.shape)\n",
    "\n",
    "\n",
    "plt.imshow(labels_arr[0])\n"
   ],
   "id": "c71da23832134852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "# labels_arr is shape (1, H, W) but find_boundaries expects (H, W)\n",
    "# Extract the first band to get 2D array\n",
    "labels_2d = labels_arr[0]  # now shape (H, W)\n",
    "\n",
    "boundaries = find_boundaries(labels_2d, mode='thick')  # binary mask of edges\n",
    "overlay = img.copy()\n",
    "overlay[boundaries] = [0, 0, 0]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(overlay)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "d75143029dbe8220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract patch features from DINOv3\n",
    "inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Check what size the processor actually created\n",
    "print(f\"Processed input shape: {inputs['pixel_values'].shape}\")  # (1, 3, H, W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state  # (1, 1 + num_patches, hidden_dim)\n",
    "\n",
    "# Remove CLS token\n",
    "patch_features = last_hidden_states[:, 1:, :]  # (1, num_patches, hidden_dim)\n",
    "batch_size, num_patches, hidden_dim = patch_features.shape\n",
    "print(f\"Total patches: {num_patches}, hidden_dim: {hidden_dim}\")\n",
    "\n",
    "# Get actual processed image dimensions from processor\n",
    "processed_h = inputs['pixel_values'].shape[2]\n",
    "processed_w = inputs['pixel_values'].shape[3]\n",
    "patch_size = model.config.patch_size\n",
    "\n",
    "# Calculate correct patch grid dimensions (may not be square!)\n",
    "num_patches_h = processed_h // patch_size\n",
    "num_patches_w = processed_w // patch_size\n",
    "expected_patches = num_patches_h * num_patches_w\n",
    "\n",
    "print(f\"Processed image: {processed_h}x{processed_w}\")\n",
    "print(f\"Patch size: {patch_size}\")\n",
    "print(f\"Calculated patch grid: {num_patches_h}x{num_patches_w} = {expected_patches} patches\")\n",
    "print(f\"Actual patches from model: {num_patches}\")\n",
    "\n",
    "# If they don't match, the image might be rectangular or have register tokens\n",
    "# Work directly with flattened patches\n",
    "patch_features_flat = patch_features[0].cpu().numpy()  # (num_patches, hidden_dim)\n",
    "\n",
    "# Create a simple spatial mask: resize label mask to approximate patch layout\n",
    "# Use the actual num_patches to guess dimensions\n",
    "if num_patches == 200:  # Common case: 10x20 or 20x10 or adjust based on aspect ratio\n",
    "    aspect_ratio = img.shape[1] / img.shape[0]  # W/H\n",
    "    num_patches_h = int(np.sqrt(num_patches / aspect_ratio))\n",
    "    num_patches_w = num_patches // num_patches_h\n",
    "    print(f\"Adjusted patch grid based on aspect ratio: {num_patches_h}x{num_patches_w}\")\n",
    "\n",
    "# Verify\n",
    "if num_patches_h * num_patches_w != num_patches:\n",
    "    print(f\"WARNING: Grid mismatch! Using flattened approach instead.\")\n",
    "    # Fallback: cluster ALL patches, no spatial filtering\n",
    "    features_inside = patch_features_flat\n",
    "else:\n",
    "    # Reshape to spatial grid\n",
    "    patch_features_2d = patch_features_flat.reshape(num_patches_h, num_patches_w, hidden_dim)\n",
    "\n",
    "    # Downsample label mask to match the patch grid\n",
    "    from skimage.transform import resize\n",
    "    mask_patches = resize(labels_2d.astype(float) > 0,\n",
    "                         (num_patches_h, num_patches_w),\n",
    "                         order=0, anti_aliasing=False) > 0.5\n",
    "\n",
    "    # Extract features only from patches inside boundaries\n",
    "    features_inside = patch_features_2d[mask_patches]\n",
    "\n",
    "print(f\"Features to cluster: {features_inside.shape}\")\n",
    "\n",
    "# Cluster with k-means\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(features_inside)\n",
    "\n",
    "# Create cluster map at patch resolution\n",
    "if num_patches_h * num_patches_w == num_patches:\n",
    "    cluster_map = np.zeros((num_patches_h, num_patches_w), dtype=int)\n",
    "    cluster_map[mask_patches] = cluster_labels + 1\n",
    "\n",
    "    # Upsample to original image size\n",
    "    from skimage.transform import resize\n",
    "    cluster_map_full = resize(cluster_map.astype(float),\n",
    "                              (img.shape[0], img.shape[1]),\n",
    "                              order=0, preserve_range=True, anti_aliasing=False).astype(int)\n",
    "else:\n",
    "    # Fallback: create a simple visualization\n",
    "    cluster_map_full = np.zeros((img.shape[0], img.shape[1]), dtype=int)\n",
    "    cluster_map_full[labels_2d > 0] = np.random.choice(cluster_labels, size=(labels_2d > 0).sum())\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(labels_2d > 0, cmap='gray')\n",
    "plt.title(\"Label Boundaries\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(cluster_map_full, cmap='tab10')\n",
    "plt.title(f\"DINOv3 Clusters (k={n_clusters})\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unique clusters: {np.unique(cluster_labels)}\")\n"
   ],
   "id": "16354f92fc970db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "99934c6a2f12b14f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.ndimage import binary_dilation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features_from_image(image_array, model, processor, device):\n",
    "    \"\"\"Extract DINOv3 patch features from an image.\"\"\"\n",
    "    inputs = processor(images=image_array, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Remove CLS token\n",
    "    patch_features = last_hidden_states[:, 1:, :]  # (1, num_patches, hidden_dim)\n",
    "    patch_features_flat = patch_features[0].cpu().numpy()  # (num_patches, hidden_dim)\n",
    "\n",
    "    # Get processed dimensions\n",
    "    processed_h = inputs['pixel_values'].shape[2]\n",
    "    processed_w = inputs['pixel_values'].shape[3]\n",
    "    patch_size = model.config.patch_size\n",
    "\n",
    "    num_patches_h = processed_h // patch_size\n",
    "    num_patches_w = processed_w // patch_size\n",
    "\n",
    "    # Reshape to spatial grid\n",
    "    patch_features_2d = patch_features_flat.reshape(num_patches_h, num_patches_w, -1)\n",
    "\n",
    "    return patch_features_2d, num_patches_h, num_patches_w\n",
    "\n",
    "\n",
    "def propagate_from_seed_points(image, seed_points, model, processor, device,\n",
    "                                 similarity_threshold=0.7, max_iterations=50):\n",
    "    \"\"\"\n",
    "    Segment image by propagating from seed points using feature similarity.\n",
    "\n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        seed_points: List of (row, col) tuples marking seed pixels\n",
    "        model: DINOv3 model\n",
    "        processor: Image processor\n",
    "        device: torch device\n",
    "        similarity_threshold: Cosine similarity threshold for expansion (0-1)\n",
    "        max_iterations: Max iterations for region growing\n",
    "\n",
    "    Returns:\n",
    "        mask: Binary mask (H, W) of segmented region\n",
    "    \"\"\"\n",
    "    # Extract patch features\n",
    "    patch_features, num_patches_h, num_patches_w = extract_features_from_image(\n",
    "        image, model, processor, device\n",
    "    )\n",
    "    H, W = image.shape[:2]\n",
    "    hidden_dim = patch_features.shape[2]\n",
    "\n",
    "    # Map seed points to patch coordinates\n",
    "    patch_h_scale = num_patches_h / H\n",
    "    patch_w_scale = num_patches_w / W\n",
    "\n",
    "    seed_patches = []\n",
    "    for row, col in seed_points:\n",
    "        patch_row = int(row * patch_h_scale)\n",
    "        patch_col = int(col * patch_w_scale)\n",
    "        patch_row = np.clip(patch_row, 0, num_patches_h - 1)\n",
    "        patch_col = np.clip(patch_col, 0, num_patches_w - 1)\n",
    "        seed_patches.append((patch_row, patch_col))\n",
    "\n",
    "    print(f\"Seed pixels: {seed_points}\")\n",
    "    print(f\"Seed patches: {seed_patches}\")\n",
    "\n",
    "    # Get seed features (average if multiple seeds)\n",
    "    seed_features = np.array([patch_features[r, c] for r, c in seed_patches])\n",
    "    seed_feature_mean = seed_features.mean(axis=0, keepdims=True)  # (1, hidden_dim)\n",
    "\n",
    "    # Compute cosine similarity of all patches to seed\n",
    "    patch_features_flat = patch_features.reshape(-1, hidden_dim)  # (num_patches, hidden_dim)\n",
    "    similarities = cosine_similarity(patch_features_flat, seed_feature_mean)[:, 0]  # (num_patches,)\n",
    "    similarity_map = similarities.reshape(num_patches_h, num_patches_w)  # (H_p, W_p)\n",
    "\n",
    "    # Initialize mask from seeds\n",
    "    mask_patches = np.zeros((num_patches_h, num_patches_w), dtype=bool)\n",
    "    for r, c in seed_patches:\n",
    "        mask_patches[r, c] = True\n",
    "\n",
    "    # Region growing: iteratively add neighbors above threshold\n",
    "    for iteration in range(max_iterations):\n",
    "        # Dilate current mask to get candidates\n",
    "        candidates = binary_dilation(mask_patches) & ~mask_patches\n",
    "\n",
    "        # Check similarity of candidates\n",
    "        new_patches = candidates & (similarity_map >= similarity_threshold)\n",
    "\n",
    "        if not new_patches.any():\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "        mask_patches |= new_patches\n",
    "\n",
    "    # Upsample mask to image resolution\n",
    "    from skimage.transform import resize\n",
    "    mask_full = resize(mask_patches.astype(float), (H, W),\n",
    "                      order=0, preserve_range=True, anti_aliasing=False) > 0.5\n",
    "\n",
    "    print(f\"Segmented {mask_full.sum()} pixels ({100*mask_full.sum()/(H*W):.1f}%)\")\n",
    "\n",
    "    return mask_full, similarity_map\n",
    "\n",
    "\n",
    "# Load your new image (image B)\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "img_b_path = \"/home/mak/PycharmProjects/SegEdge/experiments/get_data_from_api/patches_mt/dop20_592000_5983000_1km_20cm.tif\"  # <--- PUT YOUR IMAGE B PATH HERE\n",
    "\n",
    "with rasterio.open(img_b_path) as src:\n",
    "    arr_b = src.read()\n",
    "    img_b = reshape_as_image(arr_b)  # converts (C,H,W) -> (H,W,C)\n",
    "\n",
    "# Define seed points on image B (row, col)\n",
    "# You can click to get coordinates or use existing ones\n",
    "seed_points = [(2500, 2500), (2600, 2400)]  # <--- ADJUST THESE TO YOUR SEED LOCATIONS\n",
    "\n",
    "# Propagate from seeds on image B\n",
    "mask_b, similarity_map_b = propagate_from_seed_points(\n",
    "    img_b, seed_points, model, processor, device,\n",
    "    similarity_threshold=0.75,  # tune this\n",
    "    max_iterations=50\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(img_b)\n",
    "axes[0].set_title(\"Image B\")\n",
    "axes[0].axis('off')\n",
    "for r, c in seed_points:\n",
    "    axes[0].plot(c, r, 'r*', markersize=15)\n",
    "\n",
    "axes[1].imshow(similarity_map_b, cmap='hot')\n",
    "axes[1].set_title(\"Feature Similarity to Seeds\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(mask_b, cmap='gray')\n",
    "axes[2].set_title(\"Propagated Mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "overlay = img_b.copy()\n",
    "overlay[mask_b] = overlay[mask_b] * 0.5 + np.array([0, 255, 0]) * 0.5  # green overlay\n",
    "axes[3].imshow(overlay.astype(np.uint8))\n",
    "axes[3].set_title(\"Overlay on Image\")\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "2525765e3390b491",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.ndimage import binary_dilation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features_from_image(image_array, model, processor, device):\n",
    "    \"\"\"Extract DINOv3 patch features from an image.\"\"\"\n",
    "    inputs = processor(images=image_array, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Remove CLS token\n",
    "    patch_features = last_hidden_states[:, 1:, :]  # (1, num_patches, hidden_dim)\n",
    "    num_patches = patch_features.shape[1]\n",
    "    patch_features_flat = patch_features[0].cpu().numpy()  # (num_patches, hidden_dim)\n",
    "\n",
    "    # Get processed dimensions\n",
    "    processed_h = inputs['pixel_values'].shape[2]\n",
    "    processed_w = inputs['pixel_values'].shape[3]\n",
    "    patch_size = model.config.patch_size\n",
    "\n",
    "    num_patches_h = processed_h // patch_size\n",
    "    num_patches_w = processed_w // patch_size\n",
    "\n",
    "    print(f\"Image processed to: {processed_h}x{processed_w}\")\n",
    "    print(f\"Patch grid: {num_patches_h}x{num_patches_w} = {num_patches_h*num_patches_w} (actual patches: {num_patches})\")\n",
    "\n",
    "    # Handle mismatch: might be rectangular or have extra tokens\n",
    "    expected = num_patches_h * num_patches_w\n",
    "    if expected != num_patches:\n",
    "        print(f\"WARNING: Expected {expected} patches but got {num_patches}. Adjusting grid...\")\n",
    "        # Try to find correct dimensions\n",
    "        aspect_ratio = processed_w / processed_h\n",
    "        num_patches_h = int(np.sqrt(num_patches / aspect_ratio))\n",
    "        num_patches_w = num_patches // num_patches_h\n",
    "\n",
    "        # If still doesn't match, try other factorizations\n",
    "        if num_patches_h * num_patches_w != num_patches:\n",
    "            for h in range(1, int(np.sqrt(num_patches)) + 1):\n",
    "                if num_patches % h == 0:\n",
    "                    num_patches_h = h\n",
    "                    num_patches_w = num_patches // h\n",
    "            print(f\"Adjusted to: {num_patches_h}x{num_patches_w}\")\n",
    "\n",
    "    # Reshape to spatial grid\n",
    "    patch_features_2d = patch_features_flat.reshape(num_patches_h, num_patches_w, -1)\n",
    "\n",
    "    return patch_features_2d, num_patches_h, num_patches_w\n",
    "\n",
    "\n",
    "def propagate_from_seed_points(image, seed_points, model, processor, device,\n",
    "                                 similarity_threshold=0.7, max_iterations=50):\n",
    "    \"\"\"\n",
    "    Segment image by propagating from seed points using feature similarity.\n",
    "\n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        seed_points: List of (row, col) tuples marking seed pixels\n",
    "        model: DINOv3 model\n",
    "        processor: Image processor\n",
    "        device: torch device\n",
    "        similarity_threshold: Cosine similarity threshold for expansion (0-1)\n",
    "        max_iterations: Max iterations for region growing\n",
    "\n",
    "    Returns:\n",
    "        mask: Binary mask (H, W) of segmented region\n",
    "    \"\"\"\n",
    "    # Extract patch features\n",
    "    patch_features, num_patches_h, num_patches_w = extract_features_from_image(\n",
    "        image, model, processor, device\n",
    "    )\n",
    "    H, W = image.shape[:2]\n",
    "    hidden_dim = patch_features.shape[2]\n",
    "\n",
    "    # Map seed points to patch coordinates\n",
    "    patch_h_scale = num_patches_h / H\n",
    "    patch_w_scale = num_patches_w / W\n",
    "\n",
    "    seed_patches = []\n",
    "    for row, col in seed_points:\n",
    "        patch_row = int(row * patch_h_scale)\n",
    "        patch_col = int(col * patch_w_scale)\n",
    "        patch_row = np.clip(patch_row, 0, num_patches_h - 1)\n",
    "        patch_col = np.clip(patch_col, 0, num_patches_w - 1)\n",
    "        seed_patches.append((patch_row, patch_col))\n",
    "\n",
    "    print(f\"Seed pixels: {seed_points}\")\n",
    "    print(f\"Seed patches: {seed_patches}\")\n",
    "\n",
    "    # Get seed features (average if multiple seeds)\n",
    "    seed_features = np.array([patch_features[r, c] for r, c in seed_patches])\n",
    "    seed_feature_mean = seed_features.mean(axis=0, keepdims=True)  # (1, hidden_dim)\n",
    "\n",
    "    # Compute cosine similarity of all patches to seed\n",
    "    patch_features_flat = patch_features.reshape(-1, hidden_dim)  # (num_patches, hidden_dim)\n",
    "    similarities = cosine_similarity(patch_features_flat, seed_feature_mean)[:, 0]  # (num_patches,)\n",
    "    similarity_map = similarities.reshape(num_patches_h, num_patches_w)  # (H_p, W_p)\n",
    "\n",
    "    # Initialize mask from seeds\n",
    "    mask_patches = np.zeros((num_patches_h, num_patches_w), dtype=bool)\n",
    "    for r, c in seed_patches:\n",
    "        mask_patches[r, c] = True\n",
    "\n",
    "    # Region growing: iteratively add neighbors above threshold\n",
    "    for iteration in range(max_iterations):\n",
    "        # Dilate current mask to get candidates\n",
    "        candidates = binary_dilation(mask_patches) & ~mask_patches\n",
    "\n",
    "        # Check similarity of candidates\n",
    "        new_patches = candidates & (similarity_map >= similarity_threshold)\n",
    "\n",
    "        if not new_patches.any():\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "        mask_patches |= new_patches\n",
    "\n",
    "    # Upsample mask to image resolution\n",
    "    from skimage.transform import resize\n",
    "    mask_full = resize(mask_patches.astype(float), (H, W),\n",
    "                      order=0, preserve_range=True, anti_aliasing=False) > 0.5\n",
    "\n",
    "    print(f\"Segmented {mask_full.sum()} pixels ({100*mask_full.sum()/(H*W):.1f}%)\")\n",
    "\n",
    "    return mask_full, similarity_map\n",
    "\n",
    "\n",
    "# Load your new image (image B)\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "img_b_path = \"/home/mak/PycharmProjects/SegEdge/experiments/get_data_from_api/patches_mt/dop20_592000_5983000_1km_20cm.tif\"  # <--- PUT YOUR IMAGE B PATH HERE\n",
    "\n",
    "with rasterio.open(img_b_path) as src:\n",
    "    arr_b = src.read()\n",
    "    img_b = reshape_as_image(arr_b)  # converts (C,H,W) -> (H,W,C)\n",
    "\n",
    "# Define seed points on image B (row, col)\n",
    "seed_points = [(1100, 1100), (2600, 2400)]  # <--- ADJUST THESE TO YOUR SEED LOCATIONS\n",
    "\n",
    "# Propagate from seeds on image B\n",
    "mask_b, similarity_map_b = propagate_from_seed_points(\n",
    "    img_b, seed_points, model, processor, device,\n",
    "    similarity_threshold=0.75,  # tune this\n",
    "    max_iterations=50\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(img_b)\n",
    "axes[0].set_title(\"Image B\")\n",
    "axes[0].axis('off')\n",
    "for r, c in seed_points:\n",
    "    axes[0].plot(c, r, 'r*', markersize=15)\n",
    "\n",
    "axes[1].imshow(similarity_map_b, cmap='hot')\n",
    "axes[1].set_title(\"Feature Similarity to Seeds\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(mask_b, cmap='gray')\n",
    "axes[2].set_title(\"Propagated Mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "overlay = img_b.copy()\n",
    "overlay[mask_b] = overlay[mask_b] * 0.5 + np.array([0, 255, 0]) * 0.5  # green overlay\n",
    "axes[3].imshow(overlay.astype(np.uint8))\n",
    "axes[3].set_title(\"Overlay on Image\")\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "b6c22e134ca9f43d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ---- Robust patch-grid utilities ----\n",
    "def factor_hw(n, ratio):\n",
    "    \"\"\"\n",
    "    Find integers (h, w) with h*w = n minimizing |(h/w) - ratio|.\n",
    "    ratio ~ processed_h / processed_w.\n",
    "    \"\"\"\n",
    "    best = (1, n)\n",
    "    best_err = float('inf')\n",
    "    for h in range(1, int(np.sqrt(n)) + 1):\n",
    "        if n % h == 0:\n",
    "            w = n // h\n",
    "            err = abs((h / w) - ratio)\n",
    "            if err < best_err:\n",
    "                best = (h, w)\n",
    "                best_err = err\n",
    "    return best  # (h, w)\n",
    "\n",
    "def extract_patch_features(image_hw3, model, processor, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      feats_hwC: (Hp, Wp, C) patch features (L2-normalized)\n",
    "      Hp, Wp: patch grid size\n",
    "      processed_hw: (Hproc, Wproc) used by the model\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image_hw3, return_tensors=\"pt\").to(device)  # (1,3,Hproc,Wproc) [web:103]\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    tokens = out.last_hidden_state  # (1, 1+N, C), first is CLS [web:103]\n",
    "    patch_tokens = tokens[:, 1:, :]  # (1, N, C) [web:103]\n",
    "    N, C = patch_tokens.shape[1], patch_tokens.shape[2]  # [web:103]\n",
    "    Hproc, Wproc = inputs[\"pixel_values\"].shape[2], inputs[\"pixel_values\"].shape[3]  # [web:118]\n",
    "\n",
    "    # Compute expected grid and fix if mismatch (e.g., non-square/crops)\n",
    "    ps = model.config.patch_size  # e.g., 16 [web:103]\n",
    "    Hp_guess, Wp_guess = Hproc // ps, Wproc // ps  # [web:118]\n",
    "    if Hp_guess * Wp_guess != N:\n",
    "        Hp, Wp = factor_hw(N, ratio=Hproc / Wproc)  # robust factorization [web:118]\n",
    "    else:\n",
    "        Hp, Wp = Hp_guess, Wp_guess  # [web:118]\n",
    "\n",
    "    feats = patch_tokens[0].cpu().numpy().reshape(Hp, Wp, C)  # (Hp,Wp,C) [web:103]\n",
    "    # L2-normalize for cosine operations\n",
    "    eps = 1e-8\n",
    "    norms = np.linalg.norm(feats, axis=2, keepdims=True) + eps  # [web:129]\n",
    "    feats = feats / norms  # (Hp,Wp,C) normalized [web:129]\n",
    "    return feats, Hp, Wp, (Hproc, Wproc)\n",
    "\n",
    "# ---- Build feature bank from Image A labels ----\n",
    "def build_positive_bank(img_a, labels_a, model, processor, device):\n",
    "    \"\"\"\n",
    "    img_a: (H,W,3) uint8/rgb\n",
    "    labels_a: (H,W) int or bool, >0 = positive region\n",
    "    Returns bank: (Na, C) normalized features from positive patches.\n",
    "    \"\"\"\n",
    "    featsA, HpA, WpA, _ = extract_patch_features(img_a, model, processor, device)  # [web:103]\n",
    "    # Downsample labels to patch grid using nearest (preserve classes)\n",
    "    maskA = resize((labels_a > 0).astype(float), (HpA, WpA),\n",
    "                   order=0, anti_aliasing=False) > 0.5  # [web:102]\n",
    "    bank = featsA[maskA]  # (Na, C) [web:129]\n",
    "    return bank  # already L2-normalized [web:129]\n",
    "\n",
    "# ---- Score Image B by kNN to bank ----\n",
    "def zero_shot_knn_score(img_b, bank, model, processor, device, k=5):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      score_full: (Hb,Wb) similarity score in [0,1], higher = more similar to bank.\n",
    "    \"\"\"\n",
    "    featsB, HpB, WpB, _ = extract_patch_features(img_b, model, processor, device)  # [web:103]\n",
    "    X = featsB.reshape(-1, featsB.shape[2])  # (Nb, C) L2-normalized [web:129]\n",
    "\n",
    "    # kNN with cosine distance (1 - cosine_similarity)\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, max(1, len(bank))), metric='cosine')  # [web:129]\n",
    "    nn.fit(bank)  # (Na, C) [web:129]\n",
    "    dists, _ = nn.kneighbors(X, return_distance=True)  # (Nb, k) [web:129]\n",
    "\n",
    "    # Convert cosine distance to similarity, average over neighbors\n",
    "    sims = 1.0 - dists  # cosine similarity per neighbor [web:129]\n",
    "    score = sims.mean(axis=1)  # (Nb,) average k-NN similarity [web:129]\n",
    "    score_map = score.reshape(HpB, WpB)  # (HpB, WpB) [web:129]\n",
    "\n",
    "    # Upsample to pixels (nearest to keep sharp regions, or bicubic if preferred)\n",
    "    Hb, Wb = img_b.shape[0], img_b.shape[1]  # [web:118]\n",
    "    score_full = resize(score_map, (Hb, Wb),\n",
    "                        order=1, preserve_range=True, anti_aliasing=True)  # [web:118]\n",
    "    # Clamp to [0,1]\n",
    "    score_full = np.clip(score_full, 0.0, 1.0)  # [web:129]\n",
    "    return score_full  # [web:129]\n",
    "\n",
    "# ---- Complete working example ----\n",
    "\n",
    "# Build feature bank from Image A's labeled region\n",
    "print(\"Building feature bank from Image A...\")\n",
    "bank = build_positive_bank(img, labels_2d, model, processor, device)\n",
    "print(f\"Bank size: {bank.shape[0]} positive patches with {bank.shape[1]} dimensions\")\n",
    "\n",
    "# Score Image B using kNN against the bank\n",
    "print(\"\\nScoring Image B...\")\n",
    "score_b = zero_shot_knn_score(img_b, bank, model, processor, device, k=5)\n",
    "print(f\"Score map shape: {score_b.shape}, range: [{score_b.min():.3f}, {score_b.max():.3f}]\")\n",
    "\n",
    "# Threshold for a binary mask (tune threshold 0.6-0.85)\n",
    "threshold = 0.3\n",
    "mask_b = score_b >= threshold\n",
    "print(f\"Mask covers {mask_b.sum()} pixels ({100*mask_b.sum()/mask_b.size:.1f}%)\")\n",
    "\n",
    "# Visualize overlay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "overlay = img_b.copy()\n",
    "overlay[mask_b] = (0.5 * overlay[mask_b] + 0.5 * np.array([0, 255, 0])).astype(overlay.dtype)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title(\"Image A (with labels)\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(img_b)\n",
    "axs[1].set_title(\"Image B\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(score_b, cmap='hot')\n",
    "axs[2].set_title(f\"kNN Similarity Score (k=5)\")\n",
    "axs[2].colorbar = plt.colorbar(axs[2].imshow(score_b, cmap='hot'), ax=axs[2])\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(overlay)\n",
    "axs[3].set_title(f\"Transferred Mask (threshold={threshold})\")\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1ea9a2297cdb1d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e1465efc7ac18afe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
